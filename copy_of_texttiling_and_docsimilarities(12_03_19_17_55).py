# -*- coding: utf-8 -*-
"""Copy of TextTiling and Docsimilarities(12-03-19 17:55).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1As9Zj7uy_VnhToQw_G57KzKyIpduywKN
"""


from gensim.summarization.summarizer import summarize
from gensim.summarization import keywords
import urllib.request
from urllib.request import urlopen
import requests
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize

fname="test4.txt"
nltk.download('stopwords')
nltk.download('punkt')
with open(fname,'r', encoding="utf8", errors='ignore') as myfile:
  text1 = myfile.read()
#print("text:")


tt=nltk.tokenize.texttiling.TextTilingTokenizer(w=20, k=10, similarity_method=0, stopwords=None, smoothing_method=[0],
                                               smoothing_width=3, smoothing_rounds=1, cutoff_policy=1, demo_mode=False)
ttt=nltk.tokenize.texttiling.TextTilingTokenizer(w=25, k=5, similarity_method=0, stopwords=None, smoothing_method=[0],
                                               smoothing_width=3, smoothing_rounds=1, cutoff_policy=1, demo_mode=True)

s, sS, d, b = ttt.tokenize(text1)
ss=tt.tokenize(text1)
sss=nltk.tokenize.texttiling.demo(text=text1)
#for i in s:
#print(s)
print(ss)
print(b)
print(len(b))
print(sss)
print("sent_tokenize:")
i=0

print(len(ss))
print(len(ss[0]))
i=0
T=[]
p=""
for i in range(len(ss)):
  for char in ss[i]:
    #print(char)
    if (char!='.'):
      p = p + str(char)
      #print(p)
    else:
      p = p + str(char)
      T.append(p)
      print(p)
      print("--------------------------")
      p=""
    
    
print(len(T))

print(T)
S=[]
k=0;p='';
for i in range(len(b)):
  if(k==len(T)):
      break
  p=p+T[k];
  #print(T[k])
  k=k+1;
  if(b[i]==1):
  #  print(p)
    S.append(p);
    p=''
   # print("----------------------------")
    
 #   print(T[k])
    k=k+1;
for j in range(len(S)):
  print(S[j])
  print("-----------------")

#@title LDA
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(2018)
import nltk
nltk.download('wordnet')

import spacy
spacy.load('en') 
from spacy.lang.en import English
parser = English()

def tokenize(text):
    lda_tokens = []
    tokens = parser(text)
    for token in tokens:
        if token.orth_.isspace():
            continue
        elif token.like_url:
            lda_tokens.append('URL')
        elif token.orth_.startswith('@'):
            lda_tokens.append('SCREEN_NAME')
        else:
            lda_tokens.append(token.lower_)
    return lda_tokens

import nltk
nltk.download('wordnet')

from nltk.corpus import stopwords 
from nltk.stem.wordnet import WordNetLemmatizer
import string
nltk.download('stopwords')
stop = set(stopwords.words('english'))
exclude = set(string.punctuation) 
lemma = WordNetLemmatizer()
def clean(doc):
    stop_free = " ".join([i for i in doc.lower().split() if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = " ".join(lemma.lemmatize(word) for word in punc_free.split())
    return normalized
print(S)
doc_clean = [clean(doc).split() for doc in S]
print(doc_clean)

from nltk import FreqDist
fdist=[]
for i in range(len(doc_clean)):
  fdist.append(FreqDist(doc_clean[i]))

import gensim
from gensim import corpora

#  print(doc)
dd=corpora.Dictionary(doc_clean)
doc_term_matrix = [dd.doc2bow(document=doc, allow_update=True, return_missing=False) for doc in doc_clean]
print(doc_term_matrix)

print(dd)
print(doc_term_matrix)

"""Coherence =sum of scores.
score=p(wi,wj)/p(wi).p(wj). [extrinsic uci measure]
p(wi,wj)-probability of co-occurence of wi,wj in a doc.
p(wi)-prob of occurence of wi in a doc.
"""

# Creating the object for LDA model using gensim library
Lda = gensim.models.ldamodel.LdaModel
from gensim.models.coherencemodel import CoherenceModel
chunk=len(S)/2
# Running and Trainign LDA model on the document term matrix.
k=5;count=0;co=0;
coherence=[]
topics=[]
while(True):
  ldamodel = Lda(doc_term_matrix,alpha='auto',num_topics=30,eta='auto',chunksize=chunk, minimum_probability=0.0, id2word = dd, passes=50)
  coherence_model_lda = CoherenceModel(model=ldamodel, texts=doc_clean, dictionary=dd, coherence='c_v')
  coherence_lda = coherence_model_lda.get_coherence()
  #print('\nCoherence Score: ', coherence_lda)
  print(count)
  coherence.append(coherence_lda)
  topics.append(k)
  if(len(coherence)>1):
    l=len(coherence)-1
    t=coherence[l]-coherence[l-1]
    if(t<0):
      t=t*(-1)
    if(t<0.05):
      if(count==0):
        c=k
      count=count+1
    else:
      count=0;
    if(count>3):
      break;
  k=k+5
print("c",c)
print(topics)
print(coherence)
# Show graph
import matplotlib.pyplot as plt
plt.plot(topics, coherence)
plt.xlabel("Num Topics")
plt.ylabel("Coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show()
ldamodel = Lda(doc_term_matrix,alpha='auto',num_topics=c,eta='auto',chunksize=chunk, minimum_probability=0.0, id2word = dd, passes=50)      
ldamodel.save('model5.gensim')

print(len(S))
#print(ldamodel.show_topic(topicid=1,topn=10))
topics = ldamodel1.print_topics(num_words=10)
for topic in topics:
    print(topic)
#for l in ldamodel[doc_term_matrix]:
#  print(l)

import scipy.stats as Ss
from scipy.stats import entropy
def jensen_shannon(query, matrix):
    # lets keep with the p,q notation above
    p = query.T # take transpose
    q = matrix.T # transpose matrix
    m = 0.5*np.add(p,q)
    l=0.5*(Ss.entropy(p,m)+Ss.entropy(q,m))
    return np.sqrt(abs(l))

def get_most_similar_documents(query,matrix,k=2):
    """
    This function implements the Jensen-Shannon distance above
    and retruns the top k indices of the smallest jensen shannon distances
    """
    sims = jensen_shannon(query,matrix) # list of jensen shannon distances
   # print(sims)
    return sims.argsort()[:k],sims

def min(a,count):
  Min=999999999
  for i in range(len(a)):
    if(i!=count):
      if a[i] < Min:
        Min = a[i]
        minIndex = i
  return minIndex;

#doc_topic_dist = ldamodel[doc_term_matrix]
doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in ldamodel[doc_term_matrix]])
count=0
for lst in ldamodel[doc_term_matrix]:
  count=count+1
  #print("Document"+str(count))
  sims=[]
  for j in ldamodel[doc_term_matrix]:
    if(1):
      new_doc_dist=np.array([[tup[1] for tup in lst]])
      doc_dist=np.array([[tup[1] for tup in j]])
      #new_doc_dist = np.array([tup[1] for tup in Lda().get_document_topics(bow=lst)])
      sim = jensen_shannon(new_doc_dist,doc_dist)
      #np.append(sims,sim[0])
      sims.append(sim[0])
      #print("sim:")
      #print(sim[0])
      #print("---------------------")
  #remove#print(min(sims,count-1))
  #print(sims.argsort()[:2])
"""  for i in sims:
    for j in range(len(i)):
      if(i[j]>1):
        print(j)
        print(i[j])
  print("----------------") """   
for i in doc_topic_dist:
  print(i)

print(doc_dist)

